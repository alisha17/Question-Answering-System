{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "testing = {}\n",
    "documents = {}\n",
    "\n",
    "# to store all documents\n",
    "\n",
    "with open('documents.json') as f:\n",
    "    docs = json.load(f) \n",
    "for doc in docs:\n",
    "    documents[doc['docid']] = doc['text']\n",
    "\n",
    "with open('testing.json') as f:\n",
    "    test_set = json.load(f)\n",
    "for test in test_set:\n",
    "#     if len(testing) < 200:\n",
    "    testing[test['question']] = [test['docid'],test['id']]\n",
    "    \n",
    "def clean_filter(sents):\n",
    "   tokens = nltk.word_tokenize(sents)\n",
    "   tokens_lowered = [item.lower() for item in tokens]\n",
    "   # Lemmatizing the words and selecting non stop words\n",
    "   tokens_filtered = [lemmatizer.lemmatize(token) for token in tokens_lowered if token not in stopwords.words('english')]\n",
    "   # Selecting only the word types\n",
    "\n",
    "   return tokens_filtered\n",
    "\n",
    "final_ques = []\n",
    "final_para = []\n",
    "final_ques_id = []\n",
    "accuracy = 0.0\n",
    "match = 0.0\n",
    "count = len(testing.keys())\n",
    "print(\"Number of testing questions\", count)\n",
    "doc_test = {}\n",
    "training_test = {}\n",
    "# Stores all the documents\n",
    "doc_test = documents \n",
    "\n",
    "i = 0.0\n",
    "\n",
    "for key in testing.keys():\n",
    "    print(key)\n",
    "    print(i)\n",
    "    i = i+1\n",
    "\n",
    "    query = key\n",
    "    final_ques.append(query)\n",
    "    final_ques_id.append(testing[key][1])\n",
    "\n",
    "    train_set = [\" \".join(clean_filter(query))]\n",
    "    \n",
    "    for key1 in doc_test.keys():        \n",
    "        # Check the number of document for the question\n",
    "        if testing[key][0] == key1:\n",
    "            for para_index, para in enumerate(doc_test[key1]):\n",
    "                clean_set = \" \".join(clean_filter(para))\n",
    "                train_set.append(' '.join(clean_filter(para)))\n",
    "\n",
    "            tfidf_matrix_train = tfidf_vectorizer.fit_transform(train_set)  # finds the tfidf score with normalization\n",
    "            a = cosine_similarity(tfidf_matrix_train[0:1], tfidf_matrix_train)\n",
    "\n",
    "            # here the first element of tfidf_matrix_train is matched with other three elements\n",
    "            updated_a = np.delete(a, 0)\n",
    "            indices = np.argmax(updated_a)\n",
    "            testing[key].append(indices)\n",
    "            training_test[key] = [testing[key][1],indices]\n",
    "            final_para.append(doc_test[key1][indices])\n",
    "            break\n",
    "\n",
    "final = [final_para, final_ques, final_ques_id]\n",
    "# accuracy = match / count * 100\n",
    "# print(accuracy)\n",
    "\n",
    "print(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questionwords = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\"]\n",
    "commonwords = [\"the\", \"a\", \"an\", \"is\", \"are\", \"were\", \".\"]\n",
    "quantifiers = [\"few\", \"little\", \"much\", \"many\"]\n",
    "time_quant = [\"young\", \"old\", \"long\", \"earliest\", \"ago\", \"next\", \"last\", \"before\", \"after\", \"year\"]\n",
    "percent = [\"percent\", \"percentage\"]\n",
    "company = [\"university\", \"company\", \"school\", \"organization\", \"organisation\", \"acquisition\"]\n",
    "\n",
    "def processquestion(qwords):\n",
    "    # Find \"question word\" (what, who, where, etc.)\n",
    "    questionword = \"\"\n",
    "\n",
    "    qwords = [item.encode('ascii', 'ignore') for item in qwords]\n",
    "\n",
    "    for (index, word) in enumerate(qwords):\n",
    "        if word.lower() in questionwords:\n",
    "            questionword = word.lower()\n",
    "            \n",
    "    print(\"question words are\", questionword)\n",
    "    \n",
    "        \n",
    "    type_of_ques = \" \"\n",
    "    \n",
    "    # Determine question type\n",
    "    if questionword in [\"who\", \"whose\", \"whom\"]:\n",
    "        type_of_ques = \"PERSON\"\n",
    "    elif questionword in [\"where\", \"which\", \"country\", \"community\", \"region\"]:\n",
    "        type_of_ques = \"PLACE\"\n",
    "    elif questionword in [\"when\"]:\n",
    "        type_of_ques = \"TIME\"\n",
    "    elif questionword == \"how\":\n",
    "        if not set(quantifiers).isdisjoint(qwords):\n",
    "            type_of_ques = \"QUANTITY\"\n",
    "        elif not set(time_quant).isdisjoint(qwords):\n",
    "            type_of_ques = \"TIME\"\n",
    "    elif not set(time_quant).isdisjoint(qwords):\n",
    "         type_of_ques = \"TIME\"\n",
    "    elif not set(percent).isdisjoint(qwords):\n",
    "        type_of_ques = \"PERCENT\"\n",
    "    elif not set(company).isdisjoint(qwords):\n",
    "        type_of_ques = \"ORGANIZATION\"\n",
    "    else:\n",
    "        type_of_ques = \"MISC\"\n",
    "\n",
    "    # Trim possible extra helper verb\n",
    "    if questionword == \"which\":\n",
    "        target = qwords\n",
    "    target = qwords\n",
    "    \n",
    "    # Return question data\n",
    "    return (type_of_ques, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections \n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from corenlp import *\n",
    "\n",
    "corenlp = StanfordCoreNLP()\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "for (index,question) in enumerate(final[1]):\n",
    "    print(\"query is:\", question)\n",
    "    done = False\n",
    "            \n",
    "    qwords = nltk.word_tokenize(question.replace('?', ''))\n",
    "    questionPOS = nltk.pos_tag(qwords)\n",
    "        \n",
    "    (type_of_ques, target) = processquestion(qwords)\n",
    "    \n",
    "    print(type_of_ques)\n",
    "    \n",
    "    ques_id = final[2][index]\n",
    "    \n",
    "    # Get sentence keywords\n",
    "    searchwords = set(target).difference(commonwords)\n",
    "    diction = collections.Counter()\n",
    "        \n",
    "    split_para = nltk.sent_tokenize(final[0][index])\n",
    "                \n",
    "    for (i, sent) in enumerate(split_para):\n",
    "        print(\"inside here---1\")\n",
    "        sentwords = nltk.word_tokenize(sent)\n",
    "        wordmatches = set(filter(set(searchwords).__contains__, sentwords)) # words common in query and para\n",
    "        diction[sent] = len(wordmatches)\n",
    "                    \n",
    "    # Focus on 3 most relevant sentences\n",
    "    for (sentence, matches) in diction.most_common(3):\n",
    "                \n",
    "        print(\"inside here---2\")\n",
    "        \n",
    "        try:\n",
    "            sentence = sentence.encode(\"ascii\", errors = \"ignore\")\n",
    "            parse = json.loads(corenlp.parse(sentence))\n",
    "            sentencePOS = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "            # Check if solution is found\n",
    "            if done:\n",
    "                continue\n",
    "\n",
    "            # Check by question type\n",
    "            answer = \"\"\n",
    "            for worddata in parse[\"sentences\"][0][\"words\"]:  \n",
    "                if type_of_ques == \"PERSON\":\n",
    "                    if worddata[1][\"NamedEntityTag\"] == \"PERSON\":\n",
    "                        if answer == \"\":\n",
    "                            answer = worddata[0]\n",
    "                        else:\n",
    "                            answer = answer + \" \" + worddata[0]\n",
    "                        done = True\n",
    "                    elif done:\n",
    "                        break\n",
    "\n",
    "                if type_of_ques == \"PLACE\":\n",
    "                    if worddata[1][\"NamedEntityTag\"] == \"LOCATION\":\n",
    "                        if answer == \"\":\n",
    "                            answer = worddata[0]\n",
    "                        else:\n",
    "                            answer = answer + \" \" + worddata[0]\n",
    "                        done = True\n",
    "                    elif done:\n",
    "                        break\n",
    "\n",
    "                elif type_of_ques == \"QUANTITY\":\n",
    "                    if worddata[1][\"NamedEntityTag\"] == \"NUMBER\":\n",
    "                        if answer == \"\":\n",
    "                            answer = worddata[0]\n",
    "                        else:\n",
    "                            answer = answer + \" \" + worddata[0]\n",
    "                        done = True\n",
    "                    elif done:\n",
    "                        break\n",
    "\n",
    "                elif type_of_ques == \"TIME\":\n",
    "                    if worddata[1][\"NamedEntityTag\"] == \"DATE\":\n",
    "                        if answer == \"\":\n",
    "                            answer = worddata[0]\n",
    "                        else:\n",
    "                            answer = answer + \" \" + worddata[0]\n",
    "                        done = True\n",
    "                    elif done:\n",
    "                        answer = answer + \" \" + worddata[0]\n",
    "                        break\n",
    "\n",
    "                elif type_of_ques == \"PERCENT\":\n",
    "                    if worddata[1][\"NamedEntityTag\"] == \"PERCENT\":\n",
    "                        if answer == \"\":\n",
    "                            answer == worddata[0]\n",
    "                        else:\n",
    "                            answer = answer + \" \" + worddata[0]\n",
    "                        done = True\n",
    "                    elif done:\n",
    "                        answer = answer + \" \" + worddata[0]\n",
    "                        break\n",
    "                        \n",
    "                elif type_of_ques == \"ORGANIZATION\":\n",
    "                    if worddata[1][\"NamedEntityTag\"] == \"ORGANIZATION\":\n",
    "                        if answer == \"\":\n",
    "                            answer == worddata[0]\n",
    "                        else:\n",
    "                            answer = answer + \" \" + worddata[0]\n",
    "                        done = True\n",
    "                    elif done:\n",
    "                        answer = answer + \" \" + worddata[0]\n",
    "                        break\n",
    "             \n",
    "            if not done:\n",
    "                (answer, matches) = diction.most_common(1)[0]\n",
    "        \n",
    "        except Exception:\n",
    "                pass\n",
    "\n",
    "    answer = answer.encode('ascii', 'ignore')\n",
    "    answer = answer.lower()\n",
    "    answer = answer.strip()\n",
    "    \n",
    "    ques_id = (str(ques_id)).encode('ascii', 'ignore')\n",
    "\n",
    "    print(\"The answer is\", answer.encode('ascii', 'ignore'))\n",
    "    final_results[question.encode('ascii', 'ignore')] = [answer, ques_id]\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "\n",
    "for item in range(len(final_results)):\n",
    "    for k,v in final_results.iteritems():\n",
    "        new_dict.update({int(v[1]): v[0]})\n",
    "\n",
    "import collections\n",
    "\n",
    "od = collections.OrderedDict(sorted(new_dict.items()))\n",
    "\n",
    "import csv\n",
    "with open('results.csv', \"wb\") as csv_file:\n",
    "    writer = csv.writer(csv_file, skipinitialspace=True, delimiter=',', quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "    \n",
    "    writer.writerow([\"id\", \"answer\"])\n",
    "    \n",
    "    for key,value in od.iteritems():\n",
    "        writer.writerow([key, value.replace(\",\", \"\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
